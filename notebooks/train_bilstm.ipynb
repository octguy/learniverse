{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install underthesea pyvi -q\n",
    "    \n",
    "    # Set paths for Colab\n",
    "    BASE_PATH = '/content/drive/MyDrive/learniverse-ai'\n",
    "else:\n",
    "    # Local paths\n",
    "    BASE_PATH = '..'  # Assuming notebook is in notebooks/\n",
    "\n",
    "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Local Environment'}\")\n",
    "print(f\"Base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5755383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    multilabel_confusion_matrix\n",
    ")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(BASE_PATH, 'src'))\n",
    "\n",
    "from preprocessing.text_preprocessor import VietnameseTextPreprocessor\n",
    "from models.bilstm import BiLSTMClassifier, MultiLabelFocalLoss, get_model_config\n",
    "from models.dataset import Vocabulary, CommentDataset, collate_fn\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d95241",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data\n",
    "\n",
    "### Expected Data Format\n",
    "\n",
    "**ViHSD:**\n",
    "- Columns: `free_text`, `label_id` (0=CLEAN, 1=OFFENSIVE, 2=HATE)\n",
    "\n",
    "**ViCTSD:**\n",
    "- Columns: `Comment`, `Constructive`, `Toxic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_RAW_PATH = Path(BASE_PATH) / 'data' / 'raw'\n",
    "DATA_PROCESSED_PATH = Path(BASE_PATH) / 'data' / 'processed'\n",
    "MODELS_PATH = Path(BASE_PATH) / 'data' / 'models'\n",
    "\n",
    "# Create directories if needed\n",
    "DATA_PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Raw data path: {DATA_RAW_PATH}\")\n",
    "print(f\"Processed data path: {DATA_PROCESSED_PATH}\")\n",
    "print(f\"Models path: {MODELS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available files in raw data folder\n",
    "print(\"Files in raw data folder:\")\n",
    "if DATA_RAW_PATH.exists():\n",
    "    for f in DATA_RAW_PATH.iterdir():\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    print(\"  Raw data folder not found!\")\n",
    "    print(f\"  Please create: {DATA_RAW_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vihsd(data_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ViHSD dataset.\n",
    "    Expected files: vihsd_train.csv, vihsd_dev.csv, vihsd_test.csv\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        file_path = data_path / f'vihsd_{split}.csv'\n",
    "        if file_path.exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['split'] = split\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {split}: {len(df)} samples\")\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "    \n",
    "    if not dfs:\n",
    "        return None\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Rename columns to standard format\n",
    "    combined = combined.rename(columns={\n",
    "        'free_text': 'text',\n",
    "        'label_id': 'label'\n",
    "    })\n",
    "    \n",
    "    # Map labels: 0=CLEAN, 1=OFFENSIVE, 2=HATE\n",
    "    # Our labels: toxic_offensive (1 or 2), hate_speech (2 only)\n",
    "    combined['toxic_offensive'] = (combined['label'] >= 1).astype(int)\n",
    "    combined['hate_speech'] = (combined['label'] == 2).astype(int)\n",
    "    combined['source'] = 'vihsd'\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_victsd(data_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load UIT-ViCTSD dataset.\n",
    "    Expected files: victsd_train.csv, victsd_dev.csv, victsd_test.csv\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        file_path = data_path / f'victsd_{split}.csv'\n",
    "        if file_path.exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['split'] = split\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {split}: {len(df)} samples\")\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "    \n",
    "    if not dfs:\n",
    "        return None\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Rename columns to standard format\n",
    "    combined = combined.rename(columns={\n",
    "        'Comment': 'text',\n",
    "        'Toxic': 'toxic_offensive'\n",
    "    })\n",
    "    \n",
    "    # ViCTSD doesn't have hate_speech, set to 0\n",
    "    combined['hate_speech'] = 0\n",
    "    combined['source'] = 'victsd'\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading ViHSD...\")\n",
    "vihsd_df = load_vihsd(DATA_RAW_PATH)\n",
    "\n",
    "print(\"\\nLoading ViCTSD...\")\n",
    "victsd_df = load_victsd(DATA_RAW_PATH)\n",
    "\n",
    "# Combine datasets\n",
    "dfs_to_combine = []\n",
    "if vihsd_df is not None:\n",
    "    dfs_to_combine.append(vihsd_df[['text', 'toxic_offensive', 'hate_speech', 'split', 'source']])\n",
    "if victsd_df is not None:\n",
    "    dfs_to_combine.append(victsd_df[['text', 'toxic_offensive', 'hate_speech', 'split', 'source']])\n",
    "\n",
    "if dfs_to_combine:\n",
    "    df = pd.concat(dfs_to_combine, ignore_index=True)\n",
    "    print(f\"\\nCombined dataset: {len(df)} samples\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No datasets found! Please add data files to data/raw/\")\n",
    "    print(\"Expected files:\")\n",
    "    print(\"  - vihsd_train.csv, vihsd_dev.csv, vihsd_test.csv\")\n",
    "    print(\"  - victsd_train.csv, victsd_dev.csv, victsd_test.csv\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0da4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "if df is not None:\n",
    "    print(\"Dataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    display(df.head(10))\n",
    "    \n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(f\"toxic_offensive: {df['toxic_offensive'].value_counts().to_dict()}\")\n",
    "    print(f\"hate_speech: {df['hate_speech'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nBy source:\")\n",
    "    print(df.groupby('source')[['toxic_offensive', 'hate_speech']].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Toxic/Offensive distribution\n",
    "    ax1 = axes[0]\n",
    "    df['toxic_offensive'].value_counts().plot(kind='bar', ax=ax1, color=['green', 'red'])\n",
    "    ax1.set_title('Toxic/Offensive Distribution')\n",
    "    ax1.set_xlabel('Label')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_xticklabels(['Clean (0)', 'Toxic/Offensive (1)'], rotation=0)\n",
    "    \n",
    "    # Hate speech distribution\n",
    "    ax2 = axes[1]\n",
    "    df['hate_speech'].value_counts().plot(kind='bar', ax=ax2, color=['green', 'red'])\n",
    "    ax2.set_title('Hate Speech Distribution')\n",
    "    ax2.set_xlabel('Label')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_xticklabels(['No (0)', 'Yes (1)'], rotation=0)\n",
    "    \n",
    "    # Source distribution\n",
    "    ax3 = axes[2]\n",
    "    df['source'].value_counts().plot(kind='bar', ax=ax3)\n",
    "    ax3.set_title('Data Source Distribution')\n",
    "    ax3.set_xlabel('Source')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eef7c3",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = VietnameseTextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    remove_phones=True,\n",
    "    remove_emojis=False,  # Keep emojis - they can indicate sentiment\n",
    "    normalize_teencode=True,\n",
    "    normalize_repeated_chars=True,\n",
    "    word_segmentation=True,  # Vietnamese word segmentation\n",
    ")\n",
    "\n",
    "print(\"Preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "if df is not None:\n",
    "    print(\"Preprocessing texts...\")\n",
    "    tqdm.pandas(desc=\"Preprocessing\")\n",
    "    \n",
    "    df['text_processed'] = df['text'].progress_apply(preprocessor.preprocess)\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nPreprocessing examples:\")\n",
    "    for i in range(min(5, len(df))):\n",
    "        print(f\"\\nOriginal:  {df.iloc[i]['text'][:100]}...\")\n",
    "        print(f\"Processed: {df.iloc[i]['text_processed'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty texts after preprocessing\n",
    "if df is not None:\n",
    "    original_len = len(df)\n",
    "    df = df[df['text_processed'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"Removed {original_len - len(df)} empty texts\")\n",
    "    print(f\"Final dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ddcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "if df is not None:\n",
    "    df['text_length'] = df['text_processed'].str.split().str.len()\n",
    "    \n",
    "    print(\"Text length statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    df['text_length'].hist(bins=50, ax=ax)\n",
    "    ax.axvline(x=128, color='r', linestyle='--', label='Max length (128)')\n",
    "    ax.set_xlabel('Text Length (tokens)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Distribution of Text Lengths')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Percentage within max length\n",
    "    max_len = 128\n",
    "    pct_within = (df['text_length'] <= max_len).mean() * 100\n",
    "    print(f\"\\n{pct_within:.1f}% of texts are within {max_len} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27e11b",
   "metadata": {},
   "source": [
    "## 3. Prepare Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "if df is not None:\n",
    "    # Use original splits if available, otherwise create new splits\n",
    "    if 'split' in df.columns and df['split'].nunique() > 1:\n",
    "        train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "        val_df = df[df['split'] == 'dev'].reset_index(drop=True)\n",
    "        test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "        print(\"Using original splits\")\n",
    "    else:\n",
    "        # Create new splits: 80% train, 10% val, 10% test\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df, test_size=0.2, random_state=SEED, \n",
    "            stratify=df['toxic_offensive']  # Stratify by main label\n",
    "        )\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=0.5, random_state=SEED\n",
    "        )\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        val_df = val_df.reset_index(drop=True)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "        print(\"Created new splits\")\n",
    "    \n",
    "    print(f\"\\nTrain: {len(train_df)} samples\")\n",
    "    print(f\"Val:   {len(val_df)} samples\")\n",
    "    print(f\"Test:  {len(test_df)} samples\")\n",
    "    \n",
    "    # Show label distribution in each split\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    for name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "        toxic_pct = split_df['toxic_offensive'].mean() * 100\n",
    "        hate_pct = split_df['hate_speech'].mean() * 100\n",
    "        print(f\"  {name}: toxic_offensive={toxic_pct:.1f}%, hate_speech={hate_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4b1d6",
   "metadata": {},
   "source": [
    "## 4. Build Vocabulary and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "MIN_FREQ = 2\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Max vocabulary size: {MAX_VOCAB_SIZE}\")\n",
    "print(f\"Min token frequency: {MIN_FREQ}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data only\n",
    "if train_df is not None:\n",
    "    print(\"Building vocabulary...\")\n",
    "    vocab = Vocabulary(max_size=MAX_VOCAB_SIZE, min_freq=MIN_FREQ)\n",
    "    vocab.build(train_df['text_processed'].tolist())\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = MODELS_PATH / 'vocab.json'\n",
    "    vocab.save(vocab_path)\n",
    "    print(f\"Vocabulary saved to: {vocab_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "LABEL_COLUMNS = ['toxic_offensive', 'hate_speech']\n",
    "\n",
    "if train_df is not None:\n",
    "    train_dataset = CommentDataset(\n",
    "        texts=train_df['text_processed'].tolist(),\n",
    "        labels=train_df[LABEL_COLUMNS].values.tolist(),\n",
    "        vocab=vocab,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    \n",
    "    val_dataset = CommentDataset(\n",
    "        texts=val_df['text_processed'].tolist(),\n",
    "        labels=val_df[LABEL_COLUMNS].values.tolist(),\n",
    "        vocab=vocab,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    \n",
    "    test_dataset = CommentDataset(\n",
    "        texts=test_df['text_processed'].tolist(),\n",
    "        labels=test_df[LABEL_COLUMNS].values.tolist(),\n",
    "        vocab=vocab,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "    print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86621807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "if train_df is not None:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2 if not IN_COLAB else 0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2 if not IN_COLAB else 0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2 if not IN_COLAB else 0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fa93a",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = get_model_config('base')  # 'small', 'base', or 'large'\n",
    "print(\"Model configuration:\")\n",
    "for k, v in model_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if vocab is not None:\n",
    "    model = BiLSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_labels=len(LABEL_COLUMNS),\n",
    "        padding_idx=vocab.pad_idx,\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(model)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Estimated size: {total_params * 4 / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "# Using Focal Loss to handle class imbalance\n",
    "criterion = MultiLabelFocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',  # Maximize F1 score\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Criterion: MultiLabelFocalLoss\")\n",
    "print(\"Optimizer: AdamW\")\n",
    "print(\"Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05096ac2",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs['logits'], labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = (outputs['probabilities'] > 0.5).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return avg_loss, f1_micro, f1_macro\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = outputs['probabilities'].cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            \n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return avg_loss, f1_micro, f1_macro, all_preds, all_probs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100804e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 10\n",
    "PATIENCE = 3  # Early stopping patience\n",
    "\n",
    "best_val_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    'train_loss': [], 'train_f1_micro': [], 'train_f1_macro': [],\n",
    "    'val_loss': [], 'val_f1_micro': [], 'val_f1_macro': []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Early stopping patience: {PATIENCE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_f1_micro, train_f1_macro = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_f1_micro, val_f1_macro, _, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_f1_macro)\n",
    "    \n",
    "    # Log metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1_micro'].append(train_f1_micro)\n",
    "    history['train_f1_macro'].append(train_f1_macro)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1_micro'].append(val_f1_micro)\n",
    "    history['val_f1_macro'].append(val_f1_macro)\n",
    "    \n",
    "    print(f\"Train - Loss: {train_loss:.4f}, F1-micro: {train_f1_micro:.4f}, F1-macro: {train_f1_macro:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, F1-micro: {val_f1_micro:.4f}, F1-macro: {val_f1_macro:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1_macro > best_val_f1:\n",
    "        best_val_f1 = val_f1_macro\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save model\n",
    "        model_path = MODELS_PATH / 'bilstm_best.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1_macro': val_f1_macro,\n",
    "            'model_config': model_config,\n",
    "            'vocab_size': len(vocab),\n",
    "        }, model_path)\n",
    "        print(f\"✓ Saved best model (F1-macro: {val_f1_macro:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training completed! Best Val F1-macro: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if history['train_loss']:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(history['train_loss'], label='Train')\n",
    "    ax1.plot(history['val_loss'], label='Val')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # F1 Score\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history['train_f1_macro'], label='Train F1-macro')\n",
    "    ax2.plot(history['val_f1_macro'], label='Val F1-macro')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_title('Training and Validation F1-macro')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df509b6",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea170d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_path = MODELS_PATH / 'bilstm_best.pt'\n",
    "if model_path.exists():\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"Val F1-macro: {checkpoint['val_f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_f1_micro, test_f1_macro, test_preds, test_probs, test_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"F1-micro: {test_f1_micro:.4f}\")\n",
    "print(f\"F1-macro: {test_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f20a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for each label\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, label_name in enumerate(LABEL_COLUMNS):\n",
    "    print(f\"\\n{label_name.upper()}:\")\n",
    "    print(classification_report(\n",
    "        test_labels[:, i], \n",
    "        test_preds[:, i],\n",
    "        target_names=['Negative', 'Positive']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66deb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for i, (label_name, ax) in enumerate(zip(LABEL_COLUMNS, axes)):\n",
    "    cm = confusion_matrix(test_labels[:, i], test_preds[:, i])\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "        xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "        yticklabels=['Actual 0', 'Actual 1']\n",
    "    )\n",
    "    ax.set_title(f'Confusion Matrix: {label_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e4488",
   "metadata": {},
   "source": [
    "## 8. Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bac428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model package for deployment\n",
    "deployment_path = MODELS_PATH / 'deployment'\n",
    "deployment_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), deployment_path / 'model_weights.pt')\n",
    "\n",
    "# Save model config\n",
    "config = {\n",
    "    'model_type': 'BiLSTMClassifier',\n",
    "    'vocab_size': len(vocab),\n",
    "    'num_labels': len(LABEL_COLUMNS),\n",
    "    'label_names': LABEL_COLUMNS,\n",
    "    'max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'padding_idx': vocab.pad_idx,\n",
    "    **model_config\n",
    "}\n",
    "\n",
    "with open(deployment_path / 'config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Copy vocabulary\n",
    "vocab.save(deployment_path / 'vocab.json')\n",
    "\n",
    "print(\"Saved deployment package:\")\n",
    "for f in deployment_path.iterdir():\n",
    "    size = f.stat().st_size / 1024  # KB\n",
    "    print(f\"  - {f.name}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "print(\"Testing inference...\")\n",
    "\n",
    "test_comments = [\n",
    "    \"Sản phẩm này tốt lắm, mình rất thích!\",\n",
    "    \"Đm thằng này ngu vl\",\n",
    "    \"Mấy thằng người Bắc toàn lừa đảo\",\n",
    "    \"Cảm ơn bạn đã chia sẻ thông tin hữu ích\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "for comment in test_comments:\n",
    "    # Preprocess\n",
    "    processed = preprocessor.preprocess(comment)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = vocab.encode(processed)\n",
    "    if len(tokens) > MAX_SEQ_LENGTH:\n",
    "        tokens = tokens[:MAX_SEQ_LENGTH]\n",
    "    \n",
    "    # Pad\n",
    "    attention_mask = [1] * len(tokens)\n",
    "    padding = MAX_SEQ_LENGTH - len(tokens)\n",
    "    tokens = tokens + [vocab.pad_idx] * padding\n",
    "    attention_mask = attention_mask + [0] * padding\n",
    "    \n",
    "    # Predict\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, mask)\n",
    "        probs = outputs['probabilities'][0].cpu().numpy()\n",
    "    \n",
    "    print(f\"\\nComment: {comment}\")\n",
    "    print(f\"  toxic_offensive: {probs[0]:.3f}\")\n",
    "    print(f\"  hate_speech: {probs[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03352",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "The trained model is saved in `data/models/deployment/`. Files:\n",
    "- `model_weights.pt` - Model weights\n",
    "- `config.json` - Model configuration\n",
    "- `vocab.json` - Vocabulary\n",
    "\n",
    "Next steps:\n",
    "1. Download these files if running on Colab\n",
    "2. Place them in your local `data/models/deployment/` folder\n",
    "3. Run the FastAPI service"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
